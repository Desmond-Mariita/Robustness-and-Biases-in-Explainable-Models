{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec22d98",
   "metadata": {},
   "source": [
    "- Original dataset is over 27GB\n",
    "- First download sample of 100 dataset from https://mega.nz/folder/0SQCmJzZ#a51rhBc4zbfe6VwGKqyrQA\n",
    "- Dataset is already prepared\n",
    "- Llava captioning was done manually from https://llava.hliu.cc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b5f73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from glob import glob\n",
    "import shutil\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import torch\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from ast import literal_eval\n",
    "import string\n",
    "from lavis.models import load_model_and_preprocess\n",
    "\n",
    "# setup device to use\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n",
    "import openai\n",
    "\n",
    "with open('api_key.txt') as f:\n",
    "    openai_api_key = f.read()\n",
    "    \n",
    "# Set OpenAI API key.\n",
    "\n",
    "openai.api_key = openai_api_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2cd1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = os.getcwd().replace('\\\\','/')\n",
    "sample_100 = f\"{root_folder}/sample_100\"\n",
    "\n",
    "# read validation data annotations file\n",
    "\n",
    "annot_df = pd.read_json(f'{sample_100}/val.jsonl', lines=True)\n",
    "annot_df = annot_df[[\"movie\",\"img_fn\",\"metadata_fn\",\"answer_likelihood\",\"answer_orig\",\"question_orig\",\n",
    "        \"rationale_orig\",\"answer_choices\",\"question\",\"rationale_choices\",\"rationale_label\"]]\n",
    "\n",
    "# Drop duplicates based on the 'movies' column\n",
    "filtered_df = annot_df.drop_duplicates(subset='movie', keep='last')\n",
    "\n",
    "def sample100(filepath):\n",
    "    try:\n",
    "        superimposed_image(filepath)\n",
    "        sample.append(str(filepath))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Create list of images to use\n",
    "\n",
    "sample = []\n",
    "\n",
    "for i in range(0, len(annot_df[\"img_fn\"]), 50):\n",
    "    sample100(annot_df[\"img_fn\"][i])\n",
    "\n",
    "def get_image_str(filepath):\n",
    "    path = re.split(\"/\", filepath)[1]\n",
    "    path = f\"{sample_100}new_{path}\"\n",
    "    return path\n",
    "\n",
    "# add objects column\n",
    "\n",
    "new_dict = dict(zip(annot_df['img_fn'],annot_df['objects']))\n",
    "objects = []\n",
    "for img in captioned_df['img_fn']:\n",
    "    objects.append(new_dict[img])\n",
    "filtered_df['objects'] = objects\n",
    "\n",
    "def get_image_str(filepath):\n",
    "    path = re.split(\"/\", filepath)[1]\n",
    "    path = os.path.splitext(path)[0]\n",
    "    return path\n",
    "\n",
    "# Create a dictionary mapping image names to captions\n",
    "\n",
    "mapping_dict = dict(zip([name + \".jpg\" for name in a], b))\n",
    "\n",
    "# Add a new column 'captions' by mapping the values based on 'image_names'\n",
    "\n",
    "filtered_df['captions'] = [get_image_str(filepath) for filepath in filtered_df['img_fn']].map(mapping_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a7ea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add \"_i\" to each item in a list\n",
    "def add_suffix_to_list(lst_str):\n",
    "    lst = literal_eval(lst_str)\n",
    "    return [f'{item}_{i}' for i, item in enumerate(lst)]\n",
    "\n",
    "# Function to convert object strings to dictionaries\n",
    "def convert_to_dict(obj_str):\n",
    "    obj_list = literal_eval(obj_str)\n",
    "    obj_dict = {i+1: item for i, item in enumerate(obj_list)}\n",
    "    return obj_dict\n",
    "\n",
    "\n",
    "def alphabetic_enumerate_dict(iterable, start=0):\n",
    "    alphabet = string.ascii_lowercase\n",
    "    result_dict = {}\n",
    "    for idx, item in enumerate(iterable, start=start):\n",
    "        quotient, remainder = divmod(idx, len(alphabet))\n",
    "        prefix = alphabet[quotient] if quotient > 0 else ''\n",
    "        key = f\"{prefix}{alphabet[remainder]}\"\n",
    "        result_dict[key] = item\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ae9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was used to superimpose the bounding boxes and polygons on the images\n",
    "\n",
    "def superimposed_image(filepath):\n",
    "    file_name = os.path.splitext(filepath)[0]\n",
    "    save_path = re.split(\"/\", file_name)[1]\n",
    "    # read json file\n",
    "    with open(f\"{movies_path}/{file_name}.json\") as file:\n",
    "        data = json.load(file)\n",
    "    # read image file\n",
    "    image = cv2.imread(f\"{movies_path}/{file_name}.jpg\")\n",
    "    \n",
    "    for i, box in enumerate(data[\"boxes\"]):\n",
    "        x1, y1, x2, y2, score = box\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "        # Get modified label for the current object\n",
    "        label = data[\"names\"][i]\n",
    "        label = f\"{label}_{i}\"\n",
    "\n",
    "        # Add label text\n",
    "        label_text = f\"Object {i}: {label}\"\n",
    "        cv2.putText(image, label_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "    \n",
    "    for polygon in data[\"segms\"]:\n",
    "        pts = np.array(polygon, dtype=np.int32)\n",
    "        pts = pts.reshape((-1, 1, 2))  # Reshape to match the required format\n",
    "        cv2.polylines(image, [pts], isClosed=True, color=(0, 0, 255), thickness=2)\n",
    "        \n",
    "    save_path = f\"{sample_100}new_{save_path}.jpg\".replace('\\\\','/')\n",
    "        \n",
    "    #cv2.imshow('Image with Bounding Boxes and Polygons', image)\n",
    "    #cv2.waitKey(0)\n",
    "    #cv2.destroyAllWindows()\n",
    "    \n",
    "    # Save the image\n",
    "    cv2.imwrite(save_path, image)\n",
    "    \n",
    "    # Save image and json file\n",
    "    shutil.copy(f\"{movies_path}/{file_name}.json\", f\"{sample_100}\")\n",
    "    shutil.copy(f\"{movies_path}/{file_name}.jpg\", f\"{sample_100}\")\n",
    "    \n",
    "\n",
    "def sample100(filepath):\n",
    "    try:\n",
    "        superimposed_image(filepath)\n",
    "        sample.append(str(filepath))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "for i in range(0, len(annot_df[\"img_fn\"]), 75):\n",
    "    sample100(annot_df[\"img_fn\"][i])\n",
    "    if len(sample) == 100:\n",
    "        break\n",
    "\n",
    "# Final dataframe\n",
    "\n",
    "filtered_df = annot_df[annot_df['img_fn'].isin(tuple(sample))]\n",
    "filtered_df = filtered_df.drop_duplicates(subset='img_fn', keep=\"last\")\n",
    "filtered_df.to_csv(\"context_df.csv\", index=False)\n",
    "\n",
    "\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "def calculate_meteor(reference, translated):\n",
    "    # Tokenize the strings into words\n",
    "    reference_tokens = reference.split()\n",
    "    translated_tokens = translated.split()\n",
    "\n",
    "    # Calculate the METEOR score\n",
    "    meteor_score_value = meteor_score([reference_tokens], translated_tokens, alpha=0.9, beta=3, gamma=0.5)\n",
    "\n",
    "    return meteor_score_value\n",
    "# Apply the function to calculate meteor score and add it as a new column 'bleu_score' to the dataframe\n",
    "\n",
    "completed_df['meteor_score'] = completed_df.apply(lambda row: calculate_meteor(row[\"rationale_orig\"],\n",
    "                                                                               row[\"rationale4_llava\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90896947",
   "metadata": {},
   "source": [
    "#### BLIP-2 Captioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05a3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loads BLIP-2 pre-trained model\n",
    "\n",
    "model, vis_processors, _ = load_model_and_preprocess(name=\"blip2_t5\",\n",
    "                                                     model_type=\"pretrain_flant5xxl\",\n",
    "                                                     is_eval=True,\n",
    "                                                     device=device)\n",
    "\n",
    "# prepare the image\n",
    "\n",
    "image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "\n",
    "\n",
    "blip_captions = []\n",
    "\n",
    "for path in filtered_df['img_fn']:\n",
    "    raw_image = Image.open(f\"{sample_100}/{path}\").convert(\"RGB\")\n",
    "    image = vis_processors[\"eval\"](raw_image).unsqueeze(0).to(device)\n",
    "    caption = model.generate({\"image\": image, \"prompt\": \"Question: Can you tell me about this image in detail? Answer:\"})\n",
    "    print(caption)\n",
    "    display(raw_image.resize((596, 437)))\n",
    "    blip_captions.append(caption)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4c776",
   "metadata": {},
   "source": [
    "#### GPT Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8710ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_prompt = \"objects ={0: 'person', 1: 'person', 2: 'person', 3: 'person', 4: 'horse', 5: 'bottle', 6: 'cup', 7: 'cup',\\\n",
    "8: 'cup', 9: 'cup', 10: 'chair', 11: 'chair', 12: 'diningtable'}\\\n",
    "['a woman is sitting at a bar with a man and a woman']\\\n",
    "# Why is 4 touching 1 on the arm?\\\n",
    "[a: [[1], 'is', 'trying', 'to', 'calm', [0], 'down', ',', 'and', 'tell', 'her', 'to', 'stay', 'where', 'she', 'is', '.'],\\\n",
    "b: [[3], 'wants', 'to', 'show', 'her', 'a', 'message', 'she', 'has', 'just', 'been', 'sent', '.'],\\\n",
    "c: ['She', 'is', 'drawing','his', 'attention', 'to', 'something', '.'],\\\n",
    "d: [[0], 'has', 'just', 'arrived', ',', [1], 'has', 'not', 'seen', 'her', 'in','awhile', 'and', 'is', 'warmly', 'greeting', 'her', '.']]\\\n",
    "She is drawing his attention to something.\\\n",
    "objects ={1: 'person', 2: 'person', 3: 'person', 4: 'person', 5: 'person', 6: 'person', 7: 'person', 8: 'person', 9: 'person',\\\n",
    "10: 'person', 11: 'person', 12: 'person', 13: 'tie', 14: 'tie', 15: 'tie', 16: 'tie', 17: 'tie', 18: 'tie',19: 'tie', 20:'tie'}\\\n",
    "['a man in a suit and a woman in a suit are sitting in a courtroom']\\\n",
    "# Why is 8 sitting at the front of the courtroom?\\\n",
    "{a: [[7], 'is', 'in', 'a', 'jury', 'and', 'is', 'examining', 'evidence', '.'],\\\n",
    "b: [[7], 'is', 'upset', 'about', 'the', 'verdict','of', 'a', 'case', '.'],\\\n",
    "c: [[7], 'is', 'providing', 'security', 'to', 'the', 'witnesses', 'and', 'to', 'the', 'defendants','as', 'well', '.'],\\\n",
    "d: [[11], 'is', 'the', 'ultimate', 'decision', 'maker', '.']}\\\n",
    "8 is providing security to the witnesses and to the defendants as well.\\\n",
    "objects =[1: 'person', 2: 'person', 3: 'bowl']\\\n",
    "['Two women are laying on the floor in a kitchen with blood splattered on the floor']\\\n",
    "# why does 2 have stuff on her face?\\\n",
    "[a:[[1], 'smells', 'really', 'bad', '.'],\\\n",
    "b: [[1], 'got', 'hit', 'in', 'the', 'face', 'with', 'some', 'food', '.'],\\\n",
    "c: [[1], 'is','trying', 'to', 'cover', 'up', 'a', 'bruise', '.'],\\\n",
    "d: [[0], 'has', 'just', 'been', 'in', 'a', 'fight', 'with', 'someone', '.']]\\\n",
    "2 got hit in the face with some food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115aa2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_prompt = \"objects ={0: 'person', 1: 'person', 2: 'person', 3: 'person', 4: 'horse', 5: 'bottle', 6: 'cup', 7: 'cup',\\\n",
    "8: 'cup', 9: 'cup', 10: 'chair', 11: 'chair', 12: 'diningtable'}\\\n",
    "The image depicts a group of people gathered in a bar setting. There are four people in the scene, with two men and two\\\n",
    "women engaged in conversation. They are all sitting around a dining table, and there are chairs placed around it.The bar\\\n",
    "is well-stocked with various types of glassware, including numerous wine glasses and cups. Some of the wine glasses are\\\n",
    "placed on the table, while others are scattered throughout the scene. There are also several cups placed around the table\\\n",
    "and on other surfaces.\\n\\nThe atmosphere appearsto be casual and social, with the group enjoying each other's company and\\\n",
    "having drinks together.\\\n",
    "# Why is 4 touching 1 on the arm?\\\n",
    "{a: [[1], 'is', 'trying', 'to', 'calm', [0], 'down', ',', 'and', 'tell', 'her', 'to', 'stay', 'where', 'she', 'is', '.'],\\\n",
    "b: [[3], 'wants', 'to', 'show', 'her', 'a', 'message', 'she', 'has', 'just', 'been', 'sent', '.'],\\\n",
    "c: ['She', 'is', 'drawing','his', 'attention', 'to', 'something', '.'],\\\n",
    "d: [[0], 'has', 'just', 'arrived', ',', [1], 'has', 'not', 'seen', 'her', 'in','awhile', 'and', 'is', 'warmly', 'greeting', 'her', '.']}\\\n",
    "She is drawing his attention to something.\\\n",
    "objects ={1: 'person', 2: 'person', 3: 'person', 4: 'person', 5: 'person', 6: 'person', 7: 'person', 8: 'person', 9: 'person',\\\n",
    "10: 'person', 11: 'person', 12: 'person', 13: 'tie', 14: 'tie', 15: 'tie', 16: 'tie', 17: 'tie', 18: 'tie',19: 'tie', 20:'tie'}\\\n",
    "The image features a group of people sitting in chairs, likely attending a formal event or a courtroom setting. There are\\\n",
    "several individuals in the scene, with some of them wearing ties. The people are seated next to each other, some facing\\\n",
    "forward while others are turned to the side or slightly away from the camera.\\n\\nThe chairs are arranged in rows, with \\\n",
    "some chairs being more prominent in the foreground and others further back in the scene. The people in the chairs appear\\\n",
    "to be paying attention to something happening in front of them, possibly a speaker or a presentation.\\\n",
    "# Why is 8 sitting at the front of the courtroom?\\\n",
    "{a: [[7], 'is', 'in', 'a', 'jury', 'and', 'is', 'examining', 'evidence', '.'],\\\n",
    "b: [[7], 'is', 'upset', 'about', 'the', 'verdict','of', 'a', 'case', '.'],\\\n",
    "c: [[7], 'is', 'providing', 'security', 'to', 'the', 'witnesses', 'and', 'to', 'the', 'defendants','as', 'well', '.'],\\\n",
    "d: [[11], 'is', 'the', 'ultimate', 'decision', 'maker', '.']}\\\n",
    "8 is providing security to the witnesses and to the defendants as well.\\\n",
    "objects =[1: 'person', 2: 'person', 3: 'bowl']\\\n",
    "['Two women are laying on the floor in a kitchen with blood splattered on the floor']\\\n",
    "# why does 2 have stuff on her face?\\\n",
    "[a:[[1], 'smells', 'really', 'bad', '.'],\\\n",
    "b: [[1], 'got', 'hit', 'in', 'the', 'face', 'with', 'some', 'food', '.'],\\\n",
    "c: [[1], 'is','trying', 'to', 'cover', 'up', 'a', 'bruise', '.'],\\\n",
    "d: [[0], 'has', 'just', 'been', 'in', 'a', 'fight', 'with', 'someone', '.']]\\\n",
    "2 got hit in the face with some food.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a8da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "def LM(prompt):\n",
    "    \n",
    "    answer = openai.ChatCompletion.create(\n",
    "        model = MODEL,\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                   ],\n",
    "        temperature = 0,)\n",
    "    \n",
    "    answer = answer['choices'][0]['message']['content']\n",
    "    \n",
    "    new_prompt = prompt + answer + \"What is the rationale for that?\"\n",
    "    \n",
    "    time.sleep(10)\n",
    "    \n",
    "    explanation = openai.ChatCompletion.create(\n",
    "        model = MODEL,\n",
    "        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": new_prompt},\n",
    "                   ],\n",
    "        temperature = 1,)\n",
    "    \n",
    "    explanation = explanation['choices'][0]['message']['content']\n",
    "    \n",
    "    return answer, explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input(i, blip=True):\n",
    "    if blip:\n",
    "        return \"objects =\" + str(objects[i]) + str(blip_captions[i]) + \" # \" + str(question_orig[i])\\\n",
    "                           + str(alphabetic_enumerate_dict(literal_eval(answer_choices[i])))\\\n",
    "                      \n",
    "    return \"objects =\" + str(objects[i]) + str(llava_captions[i]) + \" # \" + str(question_orig[i])\\\n",
    "                    + str(alphabetic_enumerate_dict(literal_eval(answer_choices[i])))\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f82da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#context_df = pd.read_csv(\"completed_6.csv\")\n",
    "\n",
    "objects = context_df[\"new_objects1\"]\n",
    "answer_choices = context_df[\"answer_choices\"]\n",
    "question_orig = context_df[\"question_orig\"]\n",
    "answer_orig = context_df[\"answer_orig\"]\n",
    "rationale_orig = context_df[\"rationale_orig\"]\n",
    "blip_captions = context_df[\"blip_captions\"]\n",
    "llava_captions = context_df[\"captions\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dcdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_blip = []\n",
    "rationale_blip = []\n",
    "answer_llava = []\n",
    "rationale_llava = []\n",
    "\n",
    "for i in tqdm(range(0,100,1)):\n",
    "    try:\n",
    "        llava_context = llava_prompt + user_input(i, blip=False)\n",
    "        blip_context = blip_prompt + user_input(i, blip=True)\n",
    "        # Get response with llava context\n",
    "        llava_response = LM(llava_context)\n",
    "        answer_llava.append(llava_response[0])\n",
    "        rationale_llava.append(llava_response[1])\n",
    "        time.sleep(10)\n",
    "        # Get response with llava context\n",
    "        blip_response = LM(blip_context)\n",
    "        answer_blip.append(blip_response[0])\n",
    "        rationale_blip.append(blip_response[1])\n",
    "        time.sleep(30)\n",
    "    except:\n",
    "        print(i)\n",
    "        raise\n",
    "        \n",
    "context_df[\"answer4_blip\"] = answer_blip\n",
    "context_df[\"rationale4_blip\"] = rationale_blip\n",
    "context_df[\"answer2_llava\"] = answer_llava\n",
    "context_df[\"rationale2_llava\"] = rationale_llava\n",
    "\n",
    "context_df.to_csv(\"completed_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1341ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f4cdc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddde045",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0fc744",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
